# 算法与数据结构

## 算法是什么？

### 算法定义

算法是有限时间内解决特定问题的一组指令或操作步骤，它具有以下特征。

- 问题是明确的，包含清晰的输入和输出定义；
- 具有可行性，能够在有限步骤、时间和内存下完成；
- 各步骤都有确定的含义，相同的输入和运行条件下，输出始终相同；

### 数据结构定义

数据结构是计算机中组织和存储数据的方式，具有以下设计目标。
- 空间占用尽量减少，节省计算机内存；
- 数据操作尽可能快速，涵盖数据访问、添加、删除、更新等；
- 提供简洁的数据表示和逻辑信息，以便使得算法高效运行；

数据结构的设计是一个始终在“权衡”的过程，满足了一方面的快捷和遍历，往往需要在另一方面做出妥协。

- 链表相较于数组，在数据添加和删除操作上更加便捷，但牺牲了数据访问速度；
- 图相较于链表，提供了更丰富的逻辑信息，但需要占用更大的内存空间；

### 数据结构与算法的关系

数据结构与算法高度相关、紧密结合，具体表现以下三个方面。
- 数据结构是算法的基石。数据结构为算法提供了结构化存储的数据，以及用于操作数据的方法。
- 算法是数据结构发挥作用的舞台，数据结构本身仅存储数据信息，结合算法才能解决特定问题。
- 算法通常可以基于不同的数据结构进行实现，但执行效率可能相差很大，选择合适的数据结构是关键。

将数据结构与算法类比成积木：

|数据结构与算法|拼装积木|
|---|---|
|输入数据|未拼装的积木|
|数据结构|积木组织形式，包括形状、大小、连接方式等|
|算法|把积木拼装成目标形态的一系列操作步骤|
|输出数据|积木模型|

数据结构与算法是独立于语言的，所以同样的算法可以用不同的语言实现。

## 如何有效学习算法与数据结构?

理解 + 多训练

推荐阅读: Outliers <<异类-不一样的成功启示录>>, 作者: 马尔科姆-格拉德威尔

### 精通一个领域

1. Chunk it up(切碎知识点)

把你所在学习的这个整体内容分块,分块去学习,然后要做好各个块之间的联系,分快学习,整体串联;

2. Deliberate practicing(刻意练习)

刻意练习

- 联系缺陷,弱点地方
- 不舒服,不爽,枯燥
- 和生活中的例子做联想: 乒乓球,台球,游戏等

3. Feedback(反馈)

- 及时
- 主动型反馈,自动去学习优秀的编码方式
- 被动型反馈,被师父提出做一些优化和改进

### 切题四件套

1. Clarification(明确题目的意思)
2. Possible solutions(所有可能的结局方案,找到最佳方案)

- compare(time/space)
- optimal(加强)

3. Coding(多写)
4. Test cases(加上一些测试的案例)

**坚持+练习**

## 算法学习路线

总体上看，可以将学习数据结构与算法的过程分为三个阶段。

1. 算法入门

我们需要熟悉各种数据结构的特点和用法，学习不同算法的原理、流程、用途和效率等方面内容。

2. 刷算法题

从热门题目开始刷，如剑指 Offer(https://leetcode.cn/studyplan/coding-interviews/)和LeetCode Hot 100(https://leetcode.cn/studyplan/top-100-liked/)，先积累至少100道题目，熟悉主流的算法问题。初次刷题时，“知识遗忘”可能是一个挑战，但请放心，这是正常的。我们可以按照“艾宾浩斯遗忘曲线”来复习题目，通常在3-5轮的重复后，就能将其牢记在心。

> 周期性回顾
> 隔段时间多次刷同一道题，形成长期记忆并加深理解。


3. 搭建知识体系

我们可以阅读算法专栏文章、解题框架和算法教材，以不断丰富知识体系。在刷题方面，可以尝试采用进阶刷题策略，按专题分类，一题多解，一解多题等，相关的刷题心得在各个社区都能找到。

> 主动总结
> 梳理内容，发现规律，搭建完整的知识体系。

## 复杂度分析

在算法设计中，我们先后追求以下两个层面的目标：

1. **找到问题解法**：算法需要在规定的输入范围内，可靠的求得问题的正确解。
2. **寻求最优解法**：同一个问题可能存在多种解法，我们希望找到尽可能高效的算法。

衡量算法优劣的两个评价指标：

- **时间效率**：算法运行速度的快慢；
- **空间效率**：算法占用空间内存的大小；

有效的评估算法效率至关重要，只有这样我们才能将各种算法进行对比，从而指导算法设计与优化过程。效率评估方法主要分为两种：实际测试、理论估算。

- **实际测试**

这种评估方式能够反映真实情况，但也存在较大局限性。一方面，难以排除测试环境的干扰因素。另一方面，展开完整测试非常耗费资源。

- **理论估算**

理论估算又被称为「渐进复杂度分析」，简称「复杂度分析」。复杂度分析体现算法运行所需的时间（空间）资源与输入数据大小之间的关系。它描述了随着输入数据大小的增加，算法执行所需时间和空间的增长趋势。

### 时间复杂度分析

相较于直接统计算法的运行时间，时间复杂度分析的特点如下：

- **时间复杂度能够有效评估算法效率**。
- **时间复杂度的推算方法更简便**。显然，运行平台和计算操作类型都与算法运行时间的增长趋势无关。因此在时间复杂度分析中，我们可以简单地将所有计算操作的执行时间视为相同的“单位时间”，从而将“计算操作的运行时间的统计”简化为“计算操作的数量的统计”，这样以来估算难度就大大降低了。
- **时间复杂度也存在一定的局限性**。通常情况下线性阶的时间复杂度比常数阶高，但当常数较大时，线性阶的算法要用优于常数阶，有些情况下，仅凭时间复杂度很难判断算法效率的高低。

时间复杂度由低到高排列：

常数阶 < 对数阶 < 线性阶 < 线性对数阶 < 平方阶 < 指数阶 < 阶乘阶

主流排序算法的时间复杂度都是O(nlogn)，例如快速排序、归并排序、堆排序等。

#### 最差、最佳、平均时间复杂度

算法的时间效率不是固定的，而是与输入数据的分布有关。
“最差时间复杂度”对应函数渐近上界，使用❍记号表示。相应地，“最佳时间复杂度”对应函数渐近下界，用Ω记号表示：

从上述示例可以看出，最差或最佳时间复杂度只出现于“特殊的数据分布”，这些情况的出现概率可能很小，并不能真实地反映算法运行效率。相比之下，平均时间复杂度可以体现算法在随机输入数据下的运行效率，用\Theta（O里有一个横线）记号来表示。

平均时间复杂度反映算法在随机数据输入下的运行效率，最接近实际应用中的算法性能。计算平均时间复杂度需要统计输入数据分布以及综合后的数学期望。

### 空间复杂度分析
算法在运行过程中使用的内存空间主要包括以下几种。

- **输入空间**：用于存储算法的输入数据。
- **暂存空间**：用于存储算法在运行过程中的变量、对象、函数上下文等数据。
- **输出空间**：用于存储算法的输出数据。

一般情况下，空间复杂度的统计范围是“暂存空间”加上“输出空间”。

暂存空间可以进一步划分为三个部分。

- **暂存数据**：用于保存算法运行过程中的各种常量、变量、对象等。
- **栈帧空间**：用于保存调用函数的上下文数据。系统在每次调用函数时都会在栈顶部创建一个栈帧，函数返回后，栈帧空间会被释放。
- **指令空间**：用于保存编译后的程序指令，在实际统计中通常忽略不计。

而=与时间复杂度不同的是，我们通常只关注**最差空间复杂度**。这是因为内存空尽是一项硬性要求，我们必须确保在所有输入数据下有足够的内存空间预留。

最差空间复杂度中的“最差”有两层含义：
1. 以最差输入数据为准；
2. 以算法运行中的峰值内存为准；

空间复杂度由低到高排列：

常数阶(O(1)) < 对数阶(O(logn)) < 线性阶(O(n)) < 平方阶(O(n^2)) < 指数阶(O(2^n))

#### 对数阶

对数阶常见于分治算法，例如归并排序，输入长度为n的数组，每次递归将数组从中点划分为两半，形成高度为logn的递归树，使用O(logn)栈帧空间。

再例如将数字转化为字符串，输入一个正整数n，它的位数为log(10)n + 1,即对应字符串长度为log(10)n + 1，因此空间复杂度为O(log(10)n + 1) = O(logn)。


## 数据结构分类

### 逻辑结构：线性与非线性

逻辑结构揭示了数据元素之间的逻辑关系。逻辑结构可被分为线性和非线性两大类。线性结构比较直观，指数居在逻辑上呈线性排列；非线性结构则相反，呈非线性排列。

- 线性数据结构：数组、链表、栈、队列、哈希表；
- 树形结构（非线性数据结构）：树、堆、哈希表；
- 网状结构（非线性数据结构）：图，元素之间是多对多的关系；

### 物理结构：连续与分散

- **连续空间存储**：存储数组的内存空间是连续的。
- **分散空间存储**：存储链表的内存空间是分散的。

值得说明的是，所有数据结构都是基于数组、链表或者二者的组合实现的。
- **基于数组可实现**：栈、队列、哈希表、树、堆、图、矩阵、张量（维度大于等于3的数组）等。
- **基于链表可实现**：栈、队列、哈希表、树、堆、图等。

基于数组实现的数据结构也被称为“静态数据结构”，这意味着此类数据结构在初始化后长度不可变。相对应的，基于链表实现的数据机构被称为“动态数据结构”，这类数据结构在初始化后，仍可以在程序运行过程中对其长度进行调整。

**基本数据结构提供了数据的“内容类型”，而数据结构提供了数据的“组织方式”。**

### 数字编码

数字是以“补码”的形式存储在计算机中的。
- 原码：我们将数字的二进制表示的最高位视为符号位，其中0表示正数，1表示负数，其余位表示数字的值。
- 反码：正数的反码与其原码相同，负数的反码是对其原码除符号为外的所有位置取反。
- 补码：正数的补码与其原码相同，负数的补码是在其反码的基础上加1。

**计算机内部的硬件电路主要是基于加法运算设计的。**这是因为加法运算相对于其他运算来说，硬件实现起来更简单，更容易进行并行化处理，运算速度更快。**通过将加法与一些基本逻辑运算结合，计算机能够实现各种其他的数字运算。**

计算机使用补码的原因：基于补码表示，计算机可以用同样的电路和操作来处理正数和负数的加法，不需要设计特殊的硬件电路来处理减法，并且无需特别处理正负零的奇歧义问题。这大大简化了硬件设计，提升了运算效率。

### 字符编码

- **Unicode**：统一字符编码，理论上可以容纳一百多万个字符，它致力于将全球范围内的字符纳入到统一的字符集之中，提供一种通用的字符集来处理和显示各种语言文字，减少因编码标准不同产生的乱码问题。

unicode会要求不同字符占用同样的存储空间，高位填0，但其实编码英文只需要1字节，英文占用空间大小将会是ascii码下的两倍，非常浪费内存空间。

- **Utf-8**

国际上使用最广泛的unicode编码方法，它是一种可变长的编码，使用1-4个字节来表示一个字符，根据字符的复杂性未变。ascoo字符只需要一个字节，拉丁字母和希腊字母需要2个字节，常用的中文字符需要3个字节，其他的一些生僻字需要4个字节。

utf-8的编码规则并不复杂，分为以下两种情况：
    - 对于长度为1的字符，将其最高位设置为0，其余7位设置为unicode码点。值得注意的是，ascii字符在unicode字符集中占据了前128个码点，也就是说，utf-8可以向下兼容ascii码，utf-8可以用来解析年代久远的ascii码文本。
    - 对于长度为n字节的字符（其中n>1）,将首个字节的高n位都设置为1，第n+1位设置为0，从第二个字节开始麻将没个字节的高2位设置为10，其他所有位用于填充字符的unicode码点。

最高n位设置为1，系统可以通过读取最高位1的个数来解析字符的长度为n。

其他所有字节的高2位设置为10，能够起到校验符的作用，在utf-8的编码规则下，不可能有字符的最高位是10，这个结论可以用反证法来证明：假设一个字符的最高位是10，说明该字符的长度为1，对应ascii码，而ascii码的最高位是0，与假设矛盾。

- **Utf-16**
使用2或4个字节来表示一个字符，所有的ascii字符和常用的非英文字符，都是2个字节表示；少数字符需要用到4个字节表示。对于2字节的字符，utf-16与unicode码点相等。

- **Utf-32**

每个字符都使用4个字节，这意味着会占用更多的空间，也别是对于ascii字符占比较高的文本。

从存储的角度看，utf-8表示英文非常🥸，只需要1个字节。使用utf-16编码某个非英文字符（如中文）会更加高效，只需要2个字节，而utf-8需要3个字节。

从兼容性的角度看，utf-8的通用性最佳。

- char类型的长度是1 byte吗？

char的长度由编程语言采用的编码方法决定。例如：Java
JS、TS、C#都是采用UTF-16编码（保存unicode码点），因此char类型的长度为2 bytes。

以上讨论的都是字符串在编程语言中的存储方式。在文件存储或网络传输中，通常会将字符编码为utf-8格式，以达到最优的兼容性和空间效率。

## 数组与链表

### 数组
1. 索引的含义本质上是内存地址的偏移量。
2. 数组的插入和删除有以下缺点：
  - **时间复杂度**：数组的插入和删除的平均时间复杂度是O(n),其中n是数组长度；
  - **丢失元素**：由于数组的长度不可变，因此在插入元素后，超出数组长度范围的元素会丢失。
  - **内存浪费**：我们可以出实话一个比较长的数组，只用前面一部分，这样在插入数据时，丢失的末尾元素是“无意义”的，但这样做也会造成部分内存空间的浪费。

3. 扩容数组

在复杂系统中，程序难以保证数组之后的内存是可用的，从而无法安全的扩展数组容量，因此在大多数编程语言中，数组的长度是不可变的。如果我们希望扩容数组，则需要重新建立一个更大的数组，然后把原数组元素依次拷贝到新数组，这是一个O(n
)的操作，在数组很大的情况下是非常耗时的。

4. 数组优点与局限性

数组存储在连续的内存空间内，且元素类型相同。这种做法包含丰富的先验信息，系统可以利用这些信息来优化数据结构的操作效率。
  - **空间效率高**：数组为数据分配了连续的内存块，无需额外的结构开销；
  - **支持随机访问**：数组允许在O(1)时间内访问任何元素；
  - **缓存局部性**：当访问数组元素时，计算机不仅会加载它，还会缓存其周围的其他数据，从而借助高速缓存来提升后续操作的执行速度。

连续空间存储是一把双刃剑，其存在以下缺点：
  - **插入与删除效率低**：当数组中元素较多时，插入与删除操作需要移动大量的元素；
  - **长度不可变**：数组在初始化后长度就固定了，扩容数组需要讲所有数据复制到新数组，开销很大；
  - **空间浪费**：如果数组分配的大小超过了实际所需，那么多余的空间就被浪费了；

5. 数组典型应用

数组是一种基础且常见的数据结构，既频繁应用在各类算法之中，也可以用于实现各种复杂的数据结构。

  - **随机访问**：如果我们想要随机抽取一些样本，那么可以用数组存储，并生成一个随机序列，根据索引实现样本的随机抽取。
  - **排序和搜索**：数组是排序和搜索算法最常用的数据结构。快速排序、归并排序、二分查找都主要在数组上进行。
  - **查找表**：当我们需要快速查找一个元素或者需要查找一个元素的对应关系时，可以使用数组作为查找表。例如我们想要实现字符到ascii码的映射，则可以将字符的ascii码值作为索引，对应的元素存放在数组中。
  - **机器学习**：神经网络中大量使用了向量、矩阵
  张量之间的线性代数运算，这些数据都是以数组的形式构建的。数组是神经网络编程中最常使用的数据结构。
  - **数据结构实现**：数组可以用于实现栈、队列、哈希表、堆、图等数据结构。例如：图的邻接矩阵实际上是一个二维数组。

### 链表

链表节点ListNode除了包含值，还需额外保存一个引用（指针）。相同数据量下，链表比数组占用更多的内存空间，但是动态内存，可灵活扩展。

链表的插入和删除的效率很高O(1)，访问节点的效率很低，要从链表的头部开始遍历查找O(n)。

#### 常见链表类型

- **单向链表**：即上述介绍的普通链表。单向链表的节点包含值和指向下一节点的引用两项数据。我们将首个节点称为头节点，将最后一个节点称为尾节点，尾节点指向空 
 。
- **环形链表**：如果我们令单向链表的尾节点指向头节点（即首尾相接），则得到一个环形链表。在环形链表中，任意节点都可以视作头节点。
- **双向链表**：与单向链表相比，双向链表记录了两个方向的引用。双向链表的节点定义同时包含指向后继节点（下一个节点）和前驱节点（上一个节点）的引用（指针）。相较于单向链表，双向链表更具灵活性，可以朝两个方向遍历链表，但相应地也需要占用更多的内存空间。

#### 链表典型应用

单向链表通常用于实现栈、队列、哈希表和图等数据结构。

- 栈与队列：当插入和删除操作都在链表的一端进行时。它表现出先进后出的特征，对应栈，当插入操作在链表的一端进行，删除操作在链表的另一端进行，它表现出先进先出的特征，对应队列；
- 哈希表：链地址法是解决哈希冲突的主流方案之一，在该方案中，所有冲突的元素都会被放到一个链表中。
- 图：邻接表是表示图的一种常用方式，在其中，图的每个顶点都与一个链表相关联，链表中的每个元素都代表与该顶点相连的其他顶点。

双向链表常被用于需要快速查找前一个和下一个元素的场景。

- **高级数据结构**：比如在红黑树，B树中，我们需要访问节点的父节点，这可以通过在继诶单中保存一个指向父节点的引用来实现，类似于双向链表。
- **浏览器历史**：在网页浏览器中，当用户点击前进或者后退按钮时，浏览器需要知道用户访问过的前一个和后一个网页，双向链表的特性使得这种操作变得简单。
- **LRU算法**：在缓存淘汰算法（LRU）中，我们需要快速找到最近最少使用的数据，以及支持快速的添加和删除节点。这时候使用双向链表就非常合适。

循环链表常被用于需要周期性操作的场景，比如操作系统的资源调度。
- **时间片轮转调度算法**：在操作系统中，时间片轮转调度算法是一种常见的cpu调度算法，它需要对一组进程进行循环，每个进程被赋予一个时间片，，当时间片用完，cpu将切换到下一个进程。这种循环的操作就可以通过吧循环链表来实现。
- **数据缓冲区**：在某些数据缓冲区的实现中，也可能会使用到循环链表，比如音频，视频播放器中，数据流可能被分成多个缓冲块饼放入一个循环链表，以便实现无缝播放。

### 列表
动态数组的数据结构，即长度可变的数组，也常被称为列表。列表基于数组实现，继承了数组的优点，并且可以在程序运行过程中动态扩容。我们可以在列表中自由地添加元素，而无需担心超过容量控制。

### 小结

1. 数据存储在栈上和存储在堆上，对时间效率和空间效率是否有影响？

存储在栈上和堆上的数据都被存储在连续内存空间内，数据操作效率是基本一致的。然后，栈和堆具有各自的特点，从而导致以下不同点。

**分配和释放效率**：栈是一块较小的内存，分配由编译器自动完成，而堆内存相对较大，可以在代码中动态分配，更容易碎片化。因此，堆上的分配和释放操作通常比栈上的慢。

**大小限制**： 栈内存相对较小，堆的大小一般受限于可用内存，因此堆更加适合大型数组。

**灵活性**：栈上的数组的大小需要在编译时确定，而堆上的数组的大小可以在运行时动态确定。

## 栈和队列

### 栈

用数组和链表能够实现栈，但是两种实现在时间效率和空间效率上都有一些区别。

**时间效率**
当入栈和出栈操作是基本数据类型时，例如int或double，有以下结论：
- 基于数组实现的栈在触发扩容时效率会降低，但由于扩容时低频操作，因此平均效率更高；
- 基于链表实现的栈可以提供更加稳定的效率表现；

**空间效率**
- 基于数组实现的栈可能会造成一定的空间浪费；
- 链表节点也需要额外存储指针，因此链表节点占用的空间相对较大；

**栈典型应用**

> 题外话：栈很适合用来做历史记录。

- **浏览器中的前进与后退，软件中的撤销与重做**。每当我们打开新的网页，浏览器会将上一个网页执行入栈，这样我们可以通过后退操作返回到上一页面，后退操作实际上是在执行出栈。如果要同时支持后退和前进，那么需要两个栈来配合实现（后退出来的进入前进栈）。
- **程序内存管理**。每次调用函数时，系统都会在栈顶添加一个栈帧，用于记录函数的上下文信息。在递归函数中，向下递推阶段不断执行入栈操作，而向上回溯阶段会执行出栈操作。

### 队列

#### 队列典型应用

- **淘宝订单**。购物者下单后，订单将其加入队列中，系统随后会根据顺序依次处理队列中的订单，在双十一期间，短时间内产生海量订单，高并发成为工程师们需要重点攻克的问题。
- **各类待办事项**。任何需要实现“先来后到”功能的场景，例如打印机的任务队列，餐厅的出餐队列等。队列在这些场景中可以有效的维护处理顺序。

### 双向队列
#### 双向队列典型应用
双向队列兼具栈与队列的逻辑，因此它可以实现这两者的所有应用场景，同时提供更高的自由度。

撤销重做出了可以使用两个队列或者栈，也可以使用双向队列来实现。

## 哈希表

### 简介

我们可以通过哈希函数的到该key对应的键值对在数组中的存储位置。输入一个key，哈希函数的计算过程分为以下两步：
1. 通过某种哈希算法hash()计算得到哈希值；
2. 将哈希值对桶数量（数组长度）capacity取模，从而获取该key对应的数组索引index。
 
index = hash(key) % capacity;

随后，我们就可以利用index在哈希表中访问对应的桶，从而获取value。

### 哈希冲突

我们将多个输入对应统一输出的情况称为「哈希冲突」。
```
12836 % 100 = 36
20336 % 100 = 36
```
容易想到，哈希表容量n越大，多个key被分配到同一个桶中的概率就越低，冲突就越少，因此，可以通过扩容哈希表来减少哈希冲突。

类似于数组扩容，哈希表扩容需将所有键值对从原哈希表迁移至新哈希表，非常耗时。并且由于哈希表容量capacity改变，我们需要通过哈希函数来重新计算所有键值对的存储位置，进一步提高扩容过程的计算开销。为此，编程语言通常会预留足够大的哈希表容量，防止频繁扩容。

**「负载因子」**是哈希表中一个重要概念，其定义为哈希表元素数量/桶数量，用于衡量哈希冲突的严重程度，也常被作为哈希表扩容的触发条件。在java中，当负载因子超过0.75，系统会将下哈希表容量扩展为原来的2倍。

扩容是简单粗暴的解决哈希冲突的方式，但是开销很大、效率太低。可以采用以下策略：
1. 改良哈希表数据结构，**使得哈希表可以在存在哈希冲突时正常工作**。
2. 仅在必要时，即当哈希冲突比较严重时，才执行扩容操作。

哈希表的结构改良方法主要包括“链式地址”和“开放寻址”。

#### 链式地址

将多有发生冲突的键值对存储在统一链表中（每个桶存储一条链表，其中包含所有冲突元素）。基于链式地址实现的哈希表的操作方法也发生变化：

- **查询元素**：输入key，经过哈希函数得到桶索引，即可访问链表头节点，然后遍历链表并对比key以查找目标键值对。
- **添加元素**：先通过哈希函数访问链表头节点，然后将节点（即键值对）添加到链表中。
- **删除元素**：根据哈希函数的结果访问到链表头部，然后遍历链表找到目标节点，将其删除。

链式地址有以下局限性。
- **占用空间增大**：链表包含节点指针，它相比数组更加耗费内存空间。
- **查询效率低**：需要线性遍历链表来查找对应元素。

#### 开放寻址

「开放寻址」不引入额外的数据结构，而是通过“多次探测”来处理哈希冲突，探测方式主要包括线性探测，平方探测，多次哈希等。
##### 线性探测
1. 线性探测
线性探测采用固定步长的线性探索来进行探测，其操作方法与普通哈希表有所不同。
  - **插入元素**：通过哈希表计算桶索引，若发现桶内已有元素，则从冲突位置向后线性遍历（步长通常为1），直到找到空桶，将元素插入其中；
  - **查找元素**：若发现哈希冲突，则使用相同步长向后线性遍历，直到找到对应元素，返回value即可；如果遇到空桶，说明目标元素不在哈希表中，返回None；

2. 线性探测存在的问题

  - **线性探测容易产生聚焦现象**

数组中连续被占用的为位置越长，这些连续位置发生哈希冲突的可能性越大，从而进一步促使该位置的聚堆生长，形成恶性循环，最终导致增删改查操作效率劣化。

  - **不能在开放寻址哈希表中直接删除元素**

因为删除元素会在数组内产生一个空桶None，而当查询元素时，线性探测到该空桶就会返回，因此在该空桶之下的元素都无法再被访问到，程序会误判这些元素不存在。

为了解决这希问题，采用「懒删除」机制：它不直接从哈希表中移除元素，而是利用一个常量TOMBSTONE来标记桶，Nonde和TOMBSTONE都代表空桶，线性探测遇到TOMBSTONE不会终止遍历。

懒删除可能会加速哈希表的性能退化，每次删除操作都会产生一个删除标记，随着TOMBSTONE的增加，搜索时间也会增加，因为要跳过多个TOMBSTONE才能找打目标元素。

因此，考虑在线性探测中记录遇到的首个TOMBSTONE的索引，并将搜索到的目标元素与该TOMBSTONE交换位置。这样做的好处就是当每次查询或者添加元素时，元素会被移动至距离理想位置更近的桶，从而优化查询效率。

##### 平方探测

平方探测与线性探测蕾丝，都是开放寻址的常见策略之一，当发生冲突时，平方探测不是简单跳过一个固定的步数，而是跳过“探测次数的平方”的步数，1，4，9...。

平方探测主要有以下优势：
- 通过跳过平方的距离，试图缓解线性探测的聚集效应；
- 会跳过更大的距离来寻找空位置，有助于数据分布的更加均匀；

缺点：
- 仍然然存在聚集现象，即某些位置比其他位置更容易被占用。
- 由于平方的增长，平方探测可能不会探测整个哈希表，这意味着即使哈希表中有空桶，也可能无法访问到它。

##### 多次哈希

多次哈希使用多个哈希函数f1(x), f2(x),f3(x)...进行探测
- 插入元素：若哈希函数f1(x)出现冲突，则尝试f2(x)，以此类推，直到找到空桶后插入元素。
- 查找元素：在相同的哈希函数顺序下进行查找，直到找到目标函数；当遇到空桶或已经尝试所有哈希函数，说明哈希表中不存在该元素，返回None。

与线性探测相比，多次哈希方法不易产生聚焦，但多个哈希函数会增加额外的计算量。

> 请注意，开放寻址（线性探测、平方探测和多次哈希）都存在“不能直接删除元素”的问题。

#### 编程语言的选择

- java采用链式地址，当hashmap内数组长达64且链表长度到达8时，链表会被转换成红黑树以提升查找性能。
- python采用开放寻址，字典dict使用伪随机数进行探测。
- golang采用链式地址，go规定每个桶最多存储8个键值对，超出容量则连接一个溢出桶。当溢出桶过多时，会执行一次特殊的等量扩容操作，以确保性能。

### 哈希算法

在上两节中，我们了解了哈希表的工作原理和哈希冲突的处理方法。然而无论是开放寻址还是链地址法，**它们只能保证哈希表可以在发生冲突时正常工作，但无法减少哈希冲突的发生**。

为了实现“既快又稳”的哈希表数据结构，哈希算法应包含以下特点：
- **确定性**：对于相同的输入，哈希算法始终产生相同的输出，确保哈希表是可靠的。
- **效率高**：计算哈希值的过程足够快，计算开销较小，哈希表的实用性越高。
- **均匀分布**：哈希算法应使得键值对平均分布在哈希表中，分布越平均，哈希冲突的概率就越低。

实际上，哈希算法不仅用于实现哈希表，还广泛用于其他领域中。
- **密码存储**：为了保护用户密码的安全， 系统不会直接存储用户的明文密码，而是存储用户密码的哈希值，当用户输入密码时，系统会对输入的密码计算哈希值，如果两者匹配，那么密码就被视为正确。
- **数据完整性检查**：数据发送方可以计算数据的哈希值并将其一同发送，接收方根据收到的数据重新计算哈希值，与收到的哈希值进行比较，如果匹配，则数据被视为完整的。

对于密码学的相关应用：为了防止从哈希值推导出原始密码等逆向工程，哈希算法需要具备更高等级的安全特性。

- **抗碰撞性**：应当极其困难找到两个不同的输入，使得他们的哈希值相同；
- **雪崩效应**：输入的微小变化应到导致输出的显著且不可预测的变化（保证逆向非常困难）；

#### 哈希算法的设计
哈希算法的设计是一个需要考虑许多因素的复杂问题。然而对于某些要求不高的场景，我们也能设计一些简单的哈希算法。

- **加法哈希**：对输入的每个字符的 ASCII 码进行相加，将得到的总和作为哈希值。
- **乘法哈希**：利用了乘法的不相关性，每轮乘以一个常数，将各个字符的 ASCII 码累积到哈希值中。
- **异或哈希**：将输入数据的每个元素通过异或操作累积到一个哈希值中。
- **旋转哈希**：将每个字符的 ASCII 码累积到一个哈希值中，每次累积之前都会对哈希值进行旋转操作。

哈希算法中都会对大质数1000000007取模，因为使用大质数取模时，可以最大化的保证哈希值的均匀分布。因为质数不会与其他数字存在公约数，可以减少因取模操作而产生的周期性模式，从而避免哈希冲突。

**常见哈希算法**

加法哈希或者异或哈希无法区分内容相同但顺序不同的字符串，这可能会加剧哈希冲突，并引发一些安全问题。

**数据结构的哈希值**
我们知道，哈希表的 key 可以是整数、小数或字符串等数据类型。编程语言通常会为这些数据类型提供内置的哈希算法，用于计算哈希表中的桶索引。以 Python 为例，我们可以调用 hash() 函数来计算各种数据类型的哈希值。

- 整数和布尔量的哈希值就是其本身。
- 浮点数和字符串的哈希值计算较为复杂，有兴趣的同学请自行学习。
- 元组的哈希值是对其中每一个元素进行哈希，然后将这些哈希值组合起来，得到单一的哈希值。
- 对象的哈希值基于其内存地址生成。通过重写对象的哈希方法，可实现基于内容生成哈希值。

> 请注意，不同编程语言的内置哈希值和计算函数的定义和方法不同。

在许多编程语言中，只有不可变对象可作为哈希表的key，假如我们将列表（动态数组）作为key，当列表的内容发生变化时，它的哈希值也随之改变，我们就无法在哈希表中查询到原先的value了。

虽然自定义对象（比如链表节点）的成员变量是可变的，但它是可哈希的，这是因为对象的哈希值通常基于内存地址生成的，即是对象的内容发生了变化，他的内存地址不变，哈希值仍然是不变的。

在不同控制台中运行程序时，输出的哈希值是不同的。这是因为 Python 解释器在每次启动时，都会为字符串哈希函数加入一个随机的盐（Salt）值。这种做法可以有效防止 HashDoS 攻击，提升哈希算法的安全性。

**哈希表的目标是将一个较大的状态空间映射到一个较小的空间，并提供O（1）的查询效率。**

## 树

### 二叉树

1. 二叉树的常用术语。

  - 「根节点 root node」：位于二叉树顶层的节点，没有父节点。
  - 「叶节点 leaf node」：没有子节点的节点，其两个指针均指向 
 。
  - 「边 edge」：连接两个节点的线段，即节点引用（指针）。
  - 节点所在的「层 level」：从顶至底递增，根节点所在层为 1 。
  - 节点的「度 degree」：节点的子节点的数量。在二叉树中，度的取值范围是 0、1、2 。
  - 二叉树的「高度 height」：从根节点到最远叶节点所经过的边的数量。
  - 节点的「深度 depth」：从根节点到该节点所经过的边的数量。
  - 节点的「高度 height」：从最远叶节点到该节点所经过的边的数量。

> 请注意，我们通常将“高度”和“深度”定义为“走过边的数量”，但有些题目或教材可能会将其定义为“走过节点的数量”。这种情况下，高度和深度都需要加1。

2. 常见二叉树类型

  - 完美二叉树（满二叉树）：所有层的节点都被完全填满。在完美二叉树中，叶节点的度为0，其余所有节点的度为2，若树的高度为h，则节点总数为2^(h + 1) - 1，呈现标准的指数级关系，反映了自然界中常见的细胞分裂现象。

  - 完全二叉树：只有最底层的节点未被填满，且最底层节点尽量靠左填充。
  - 完满二叉树：除了叶子结点之外，其余所有节点都只有两个子节点（所有节点的度都为0或2）。
  - 平衡二叉树：任意节点的左子树和右子树的高度之差的绝对值不超过1。

3. 二叉树的退化

当二叉树的每层节点都被填满时，达到“完美二叉树”。而当所有节点都偏向一侧时，二叉树退化为“链表”。
  - 完美二叉树是理想情况，可以充分发挥二叉树“分治”的优势。
  - 链表则是另一个极端，各项操作都变成线性擦欧总，时间复杂度退化至O(n)。

### 二叉树的遍历

树是一种非线性结构，这使得遍历树比遍历链表更加复杂，需要借助搜索算法来实现。二叉树的遍历方式包含层序遍历、前序遍历、中序遍历和后续遍历。

#### 层序遍历

层序遍历：从顶部到底部逐层遍历二叉树，本质上属于「广度优先遍历」，每一层从左到右顺序访问节点。

广度优先遍历通常结束“队列”来实现。队列遵循“先进先出”的规则，而广度优先遍历则遵循“逐层推进”的规则，两者背后的思想是一致的。

**复杂度分析**

- 时间复杂度（O(n)）：所有节点都被访问一次，使用O(n)时间，其中n为节点数量；
- 空间复杂度（O(n)）：在最差情况下，即满二叉树时，遍历到最底层之前，队列中最多同时存在(n + 1)/2个节点，占用O(n)空间。

#### 前序、中序、后序遍历

前序、中序、后序遍历都属于「深度优先遍历」，深度优先遍历就像绕着整个二叉树的外围走一圈，每个及诶单都会遇到三个值，分别对应前序、中序、后序遍历。
**复杂度分析**
- 时间复杂度（O(n)）：所有节点都被访问一次，使用O(n)时间，其中n为节点数量；
- 空间复杂度（O(n)）：在最差情况下，即树退化为链表，递归深度达到n，系统占用O(n)堆栈空间。

#### 用数组表示二叉树

1. 完美二叉树： 若节点的索引为i，则该节点的左子节点索引为2i+1，右子节点索引为2i+2。

映射公式的角色相当于链表中的指针，给定数组中的任意一个节点，我们都可以通过映射公式来访问它的左右子节点。

2. 针对非完美二叉树，可以给不存在的节点设置为null。从而能够使用数组来准确描述。

```
/* 二叉树的数组表示 */
// 使用 null 来表示空位
let tree = [1, 2, 3, 4, null, 6, 7, 8, 9, null, null, 12, null, null, 15];
```

值得说明的是，完全二叉树非常适合使用数组来表示。Null只会出现在最底层且靠右的位置，因此null一定出现在层序遍历序列的末尾。

3. 二叉树的数组的优点与局限性

**优点**
- 数组存储在连续的内存空间中，对缓存友好，访问与遍历速度较快；
- 不需要存储指针，比较节省空间；
- 允许随机访问界定；

**局限性**
- 数组存储需要连续的内存空间，因此不适合存储数据量过大的树；
- 增删节点需要通过数组的插入与删除操作实现，效率较低；
- 当二叉树中存在大量None时，数组中包含的节点数据比重较低，空间利用率较低；



### 二叉搜索树

1. 「二叉搜索树」满足以下条件：
  - 对于根节点，左子树中所有节点的值 < 根节点的值 < 右子树中所有节点的值。
  - 任意节点的左右子树也都是二叉搜索树
2. 二叉树中删除节点氛围三种情况：
  - 1）如果待删除的节点度为0时，表示该节点是叶节点，可以直接删除；
  - 2）当待删除节点的度为1时，将待删除节点替换为其子节点即可；
  - 3）当待删除节点的度为2时，我们无法直接删除它，而需要使用一个节点替换该节点。由于要保持二叉搜索树"左 < 根 < 右"的性质，因此这个节点可以是右子树的最小节点或左子树的最大节点。
    - a）找到待删除节点“中序遍历序列”中的下一个节点，记为tmp；
    - b）将tmp的值覆盖待删除节点的值，并在树中递归删除节点tmp；
    删除节点操作同样适用O(logn)时间，其中查找待删除节点需要O(logn)时间，获取中序遍历后继节点需要O(logn)时间。


3. 二叉搜索树的中序遍历是升序遍历。

利用中序遍历升序的性质，我们在二叉搜索树中获取有序数据仅需O(n)时间，无需额外的排序操作。

4. 二叉搜索树的常见应用

  - 用作系统中的多级索引，实现高效的查找、删除、插入操作；
  - 作为某搜索算法的底层数据结构；
  - 用于存储数据流，以保持其有序状态；


### AVL树

AVL树既是二叉搜索树又是平衡二叉树，同时满足这两类二叉树的所有性质，因此也被称为「平衡二叉搜索树」。

“节点高度”是指从该节点到最远叶节点的距离，即经过的“边”的数量。需要特别注意的是，叶节点的高度为0，而空节点的高度为-1。

“节点平衡因子”：节点左子树的高度减去右子树的高度，同时规定空节点的平衡因子为0。

> 设平衡因子为f，则一棵AVL树的任意节点的平衡因子皆满足 -1 <= f <= 1;

**AVL旋转树**
AVL树的特点在于“旋转操作”，它能够在不影响二叉树中序遍历的前提下，使失衡节点重新恢复平衡。换句话说，旋转操作既能保持“二叉搜索树”的性质，也能使树重新变为“平衡二叉树”。
**旋转操作**
旋转操作分为几种：右旋、左旋、先左旋后右旋、先右旋后左旋。

实际操作过程中，可以将需要左旋还是右旋或其他封装成一个函数，用这个函数来判断。
- 四种旋转情况的选择条件

|---|---|---|
|失衡节点的平衡因子|子节点的平衡因子|应采用的旋转方法|
|>1(即左偏树)|>=0|右旋|
|>1(即左偏树)|<0|先左旋后右旋|
|<-1(即右偏树)|<=0|左旋|
|<-1(即右偏树)|>0|先右旋后左旋|


意味着，每次插入/删除节点时都触发一次旋转，保证AVL树的平衡。
**插入节点**

AVL树的节点插入操作与二叉搜索树在主体上类似，唯一的区别是，在AVL树中插入节点后，从该节点到根节点的路径上可能会出现一系列失衡节点。因此，需要从这个节点开始，自底向上执行旋转操作，使所有失衡节点恢复平衡。

**删除节点**

AVL树的节点删除操作也需要自底向上执行旋转操作，使所有失衡节点恢复平衡。

**AVL树典型应用**
- 组织和存储大型数据，适用于高频查找，低频增删的场景；
- 用于创建数据库中的索引系统；
- 红黑树在许多应用中比AVL树更受欢迎，这是因为红黑树的平衡条件相对宽松，在红黑树中插入和删除节点所需的旋转操作相对较少，其节点增删操作的平均效率更高。

对一组输入数据构建二叉树，通常选择中点元素作为根节点，再递归的构建左右子树，这样可以最大程度保证树的平衡性。

对于节点数量为n的平衡二叉树，高度为 log(n+1) - 1,低层节点数量为 （n+1）/ 2。
## 堆

1. 概念
「堆」是一种满足特定条件的完全二叉树，主要可分为两种类型
- 「大顶堆」：任意节点的值大于等于其子节点的值；
- 「小顶堆」：任意节点的值小于等于其子节点的值；

堆作为完全二叉树的一个特例，具有以下特性：

- 最底层节点靠左填充，其他层的节点都被填满。
- 我们将二叉树的根节点称为“堆顶”，将底层最靠右的节点称为“堆底”。
- 对于大顶堆（小顶堆），堆顶元素（即根节点）的值分别是最大（最小）的。

堆通常用作实现优先级队列，大顶堆相当于元素从大到小顺序出队的优先队列。

2. 堆的操作

**元素入堆**

给定元素val，我们首先将其添加到堆底。添加后，由于val可能大于堆中的其他元素，堆的成立条件可能已被破坏，需要修复从插入节点到根节点路径上的各个节点，这个操作被称为「堆化」。需要从入堆节点开始，从底至顶执行堆化。堆化完成后，最大堆的性质得到修复。

树的高度为O(logn)，堆化操作的循环轮数最多为O(logn)，元素入堆操作的时间复杂度为O(logn)。

**堆顶元素出堆**

堆顶元素是二叉树的根节点，即列表首元素。如果直接从列表中删除首元素，那么二叉树中所有节点的索引都会发生变化，这使得后续使用堆化修复变得困难（其实我理解是因为直接删除的方案时间复杂度比较高O(n)）。为了尽量减少元素索引的变动，我们采用以下操作步骤。

- 1）交换堆顶元素与堆底元素（即交换根节点与最右叶节点）；
- 2）交换完成后，将堆底从列表中删除；
- 3）从根节点开始，从顶至底堆化；

3. 堆常见应用

  - **优先队列**：堆通常作为实现优先队列的首选数据结构，其入队和出队操作的时间复杂度均为O(logn)，而建队操作为O(n)，这些操作都非常高效。
  - **堆排序**：给定一组数据，我们可以用它建立一个堆，然后不断的执行元素出堆擦欧总，从而得到有序数据。
  - **获取最大的k个元素**：这是一个经典的算法问题，同时也是一种典型应用，如销量前10的商品等（解决方案就是构建一个大顶堆，然后返回数组的前十个数据？）。

4. 建堆操作

首先创建一个空堆，然后遍历列表，一次对每个元素执行“入堆操作”，即先将元素添加至堆的尾部，再对该元素执行“从底至顶堆化”。

由于节点是从顶到底依次被添加进二叉树的（总是先添加到堆底，最右边的叶子结点，然后再进行堆化操作），因此堆是“自上而下”的构建的。

设元素数量为n，每个元素的入堆操作使用O(logn)，建堆方法的时间复杂度是O(nlogn)。

**更高效的建堆方法**

  1）将列表中所有元素原封不动的添加到堆中，此时堆的性质尚未得到满足；
  2）倒序遍历堆（即层序遍历的倒序），依次对每个非叶节点执行（从顶至底堆化）。

每当堆化一个节点后，以该节点为根节点的子树就形成一个合法的子堆。倒序遍历，堆是“自下而上”被构建的。

之所以选择倒序遍历，是因为这样能够保证当前节点之下的子树已经是合法的子堆，这样可以更快的完成堆化（感觉有点分治的思想，拆分成子问题，并将子问题处理好，然后一步步合并？）

值得说明的是，叶节点没有子节点，天然就是合法的子堆，因此无需堆化。

**时间复杂度**

从字面上计算时间复杂度是O(nlogn)，但可以通过进一步的推到得出实际上的时间复杂度为O(n)，黑魔法地址：https://www.hello-algo.com/chapter_heap/build_heap/#823。

> 需要注意的是，数据机构的“堆”和内存管理的“堆”不是同一个概念，只是碰巧都叫堆。计算机内存中的堆是动态内存分配的一部分，程序在运行时可以使用它来存储数据。程序可以请求一定量的堆内存，用于存储如对象和数组等复杂结构。当这些数据不再需要时，程序需要释放这些内存，以防止内存泄漏。相较于栈内存，堆内存的管理和使用需要更谨慎，不恰当的使用可能会导致内存泄漏和野指针等问题。

## 图

### 简介
「图」是一种非线性结构，如果将顶点看作节点，将边看作连接各个节点的引用（指针），我们就可以将图看作一种是从链表拓展而来的数据结构。

相较于线性关系（链表）和分治关系（树），网络关系（图）的自由度更高，从而更复杂。

### 图的分类

**根据边是否有方向划分：**

- 无向图，微信中的“好友关系”。
- 有向图，微博上的“关注”与“被关注”关系。

**根据所有顶点是否连通划分：**

- 连通图： 从某个顶点出发，可以到达任意顶点；
- 非连通图：从某个顶点出发，至少有一个顶点无法到达；

如果为边添加“权重”变量，则得到「有权图」。系统会根据共同游戏时间来计算玩家之间的“亲密度”，可以用有权图来表示亲密度。

**图数据结构包含以下常用术语：**

- 「邻接 adjacency」：当两顶点之间存在边相连时，称这两顶点“邻接”。在图 9-4 中，顶点 1 的邻接顶点为顶点 2、3、5。
- 「路径 path」：从顶点 A 到顶点 B 经过的边构成的序列被称为从 A 到 B 的“路径”。在图 9-4 中，边序列 1-5-2-4 是顶点 1 到顶点 4 的一条路径。
- 「度 degree」：一个顶点拥有的边数。对于有向图，「入度 In-Degree」表示有多少条边指向该顶点，「出度 Out-Degree」表示有多少条边从该顶点指出。

### 图的表示

图的常用表示方式包括“邻接矩阵”和“邻接表”。

#### 邻接矩阵
邻接矩阵的特性：
- 顶点不能与自身相连，因此邻接矩阵对角线元素没有意义；
- 对于无向图，两个方向的边等价，此时邻接矩阵关于主对角线对称；
- 将邻接矩阵的元素从1和0替换为权重，则可表示有权图；

#### 邻接表

「邻接表」使用n个链表来表示图，链表节点是顶点。第i条链表对应顶点i，其中存储了该顶点的所有邻接顶点（即与该顶点相连的顶点）。

邻接表仅存储实际存在的边，而边的总数通常小于n^2，因此更加节省空间。然后，在邻接表中需要通过链表来查找边，因此其间效率不如邻接矩阵。

邻接表的结构与哈希表中的“链式寻址”非常相似，因此我们可以采用类似方法来优化效率。（可以将链表转化为AVL树或红黑树），还可以把链表转换为哈希表。

### 图常见应用

- 社交网络：用户与好友关系分别对应顶点和边。
- 地铁线路：站点与站点间的轨道分别对应顶点和边，可进行最短路线推荐。
- 太阳系：星体与星体间的万有引力作用分别对应顶点和边，可进行行星轨道计算。

### 图基础操作

#### 基于邻接矩阵的实现

给定一个顶点数量为n的无向图，各种操作如下。

- **添加或删除边**：直接在邻接矩阵中修改指定的边即可，使用O(1)时间。而由于是无向图，因此需要同时更新两个方向的边。
- **添加顶点**：在邻接矩阵的尾部添加一行一列，并全部填0即可，使用O(n)时间。
- **删除顶点**：在邻接矩阵中删除一行一列。当删除首行首列时达到最差情况，需要将 
(n - 1)^2个元素“向左上移动”，从而使用O(n^2)时间。
- **初始化**：传入n个顶点，初始化长度为n的顶点列表 vertices ，使用O(n)时间；初始化n x n大小的邻接矩阵 adjMat ，使用O(n ^ 2)时间。

#### 基于邻接表的实现
设无向图的顶点总数为n、边总数为m，各种操作如下：

- **添加边**：在顶点对应链表的末尾添加边即可，使用O(1)时间。因为是无向图，所以需要同时添加两个方向的边。
- **删除边**：在顶点对应链表中查找并删除指定边，使用O(m)时间。在无向图中，需要同时删除两个方向的边。
- **添加顶点**：在邻接表中添加一个链表，并将新增顶点作为链表头节点，使用O(1)时间。
- **删除顶点**：需遍历整个邻接表，删除包含指定顶点的所有边，使用O(n + m)时间。
- **初始化**：在邻接表中创建n个顶点和 2m条边，使用O(n + m)时间。

### 效率对比

在邻接矩阵中操作边的效率更高，只需要一次数组访问或赋值操作即可。综合来看，邻接矩阵体现了“以空间换时间”，而邻接表体现了“以时间换空间”的原则。

### 图的遍历

#### 广度优先遍历

广度优先遍历是一种由近及远的遍历方式，从某个节点出发，始终优先访问距离最近的顶点，并一层层向外扩张。以邻接表举例，从左上角顶点出发，先遍历该顶点的所有邻接顶点，然后遍历下一个顶点的所有邻接顶点，以此类推，直至所有顶点访问完毕。

**算法实现**
BFS 通常借助队列来实现。队列具有“先入先出”的性质，这与 BFS 的“由近及远”的思想异曲同工。

- 1）将遍历起始顶点 startVet 加入队列，并开启循环。
- 2）在循环的每轮迭代中，弹出队首顶点并记录访问，然后将该顶点的所有邻接顶点加入到队列尾部。
- 3）循环步骤 2. ，直到所有顶点被访问完成后结束。

> 广度优先遍历的顺序不唯一。广度优先遍历只要求按“由近及远”的顺序遍历，而多个相同距离的顶点的遍历顺序是允许被任意打乱的。

**复杂度分析**
v是顶点个数，e时边的条数。

- **时间复杂度**： 所有顶点都会入队并出队一次，使用 
O(v)时间；在遍历邻接顶点的过程中，由于是无向图，因此所有边都会被访问2次，使用O(2e)时间；总体使用O(v + e)时间。

空间复杂度： 列表 res ，哈希表 visited ，队列 que 中的顶点数量最多为v，使用O(v)空间。


#### 深度优先遍历

深度优先遍历是一种优先走到底、无路可走再回头的遍历方式。以邻接表举例，从左上角顶点出发，访问当前顶点的某个邻接顶点，直到走到尽头时返回，再继续走到尽头并返回，以此类推，直至所有顶点遍历完成。

这种“走到尽头再返回”的算法范式通常基于递归来实现，与广度优先遍历类似，在深度优先遍历中我们也需要借助一个哈希表来记录已被访问的节点，以避免重复访问节点。

> 与广度优先遍历类似，深度优先遍历序列的顺序也不是唯一的。给定某顶点，先往哪个方向探索都可以，即邻接顶点的顺序可以任意打乱，都是深度优先遍历。

**复杂度分析**
v是顶点个数，e时边的条数。
- **时间复杂度**： 所有顶点都会被访问1次，使用O(v)时间；所有边都会被访问2次，使用O(2e)时间；总体使用O(v + e)时间。

- **空间复杂度**： 列表 res ，哈希表 visited 顶点数量最多为v，递归深度最大为v，因此使用O(v)空间。

> 在邻接表中，“与该顶点相连接的所有顶点”的顶点顺序是否有要求？可以是任意顺序，但在实际应用中，可能会需要按照指定规则来排序，比如按照节点添加的次序，或者按照顶点值大小的顺序等等，这样有助于快速查找“带有某种极值”的顶点。

## 搜索

### 二分查找

#### 复杂度分析
**时间复杂度O(logn)**：在二分循环中，区间每轮缩小一半，循环次数为logn。

**空间复杂度O(1)**：指针i和j使用常数大小空间。

#### 优点与局限性

二分查找在时间和空间方面都有比较好的性能。
- 二分查找的时间效率高，在大数据量下，对数阶的时间复杂度具有显著优势。
- 二分查找无需额外空间，相较于需要借助额外空间的搜索算法（例如哈希查找），二分查找更加节省空间。

二分查找并非适用所有情况，原因如下：
- 二分查找只适用于数组，二分查找需要跳跃式的访问元素。而在链表中执行跳跃式访问效率很低，因此不适合用在链表或基于链表实现的数据结构。
- 小数据量下，线性查找性能更加。（二分查找的计算次数比线性查找多，数据量n较小时，线性查找反而比二分查找更快）。

### 二分查找插入点

二分查找无非就是给指针i和j分别设定搜索目标，目标可能是一个具体的元素（例如target），也可能是一个元素范围（例如小于target的元素）。
在不断的循环二分中，指针i和j都逐渐逼近预先设定的目标，最终，他们呢或是成功找到答案，或是越过边界后停止。

### 二分查找边界

1. 查找左边界

2. 查找右边界
两个巧妙转换：
  -  复用查找左边界：可利用查找最左元素的函数来查找最右元素，具体方法为：将查找最右一个target转换为查找最左一个taget + 1。
  - 转换为查找元素：当数组不包含target时，最终i和j分别指向首个大于、小于target的元素。
    - 查找最左一个target：可以转化为target - 0.5，并返回指针i。
    - 查找最右一个target：可以转化为target + 0.5，并返回指针j。
  使用此方法需要注意两点：

  1）给定数组不包含小数，这意味着我们无需关心如何处理相等的情况。

  2）因为该方法引入了小数，所以需要将函数中的变量target改为浮点数类型。

### 哈希优化策略

#### 线性查找：以时间换空间

通常是暴力穷举：
```
function twoSumBruteForce(nums, target) {
    const n = nums.length;
    // 两层循环，时间复杂度O(n ^ 2)
    for(let i = 0; i < n; i++) {
        for(let j = i + 1; j < n; j++) {
            if(nums[i] + nums[j] === target) {
                return [i,j]
            }
        }
    }
    return []
}
```
此方法的时间复杂度为O(n ^ 2)，空间复杂度为O(1)，在大数据量下非常耗时。


#### 哈希查找：以空间换时间
借助一个哈希表，键值对分别为数组元素和元素索引。

1. 判断数字 target - nums[i] 是否在哈希表中，若是则直接返回这两个元素的索引。
2. 将键值对 nums[i] 和索引 i 添加进哈希表。

```
function twoSumHashTable(nums, target) {
    // 辅助哈希表，空间复杂度 O(n)
    let m = {};
    // 单层循环，时间复杂度 O(n)
    for (let i = 0; i < nums.length; i++) {
        if (m[target - nums[i]] !== undefined) {
            return [m[target - nums[i]], i];
        } else {
            m[nums[i]] = i;
        }
    }
    return [];
}
```
此方法通过哈希查找将时间复杂度从O(n ^ 2)将低至O(n
)，大幅提升运行效率。
需要维护一个额外的哈希表，空间复杂度为O(n
)。**尽管如此，该方法的整体时空效率更为均衡，因此它是本题的最优解法**。

### 重识搜索算法

搜索算法可根据实现思路分为以下两类。

- **通过遍历数据结构来定位目标元素**，例如数组、链表、树和图的遍历等。
- **利用数据组织结构或数据包含的先验信息**，实现高效元素查找，例如二分查找、哈希查找和二叉搜索树查找等。

#### 暴力搜索

暴力搜索通过遍历数据结构的每个元素来定位目标元素：
- “线性搜索”适用于数组和链表等线性数据结构。它从数据结构的一端开始，逐个访问元素，直到找到目标元素或达到另一端仍没有找到目标元素为止。
- “广度优先搜索”和“深度优先搜索”是树和图的两种遍历策略，广度优先搜索从初始节点开始逐层搜索，由近及远访问各个节点。深度优先搜索是从初始节点开始，沿着一条路径走到头为止，再回溯并尝试其他路径，直到遍历整个数据结构。

暴力搜索的优点是简单且通用性好，无需对数据做预处理和借助额外的数据结构。此类算法的时间复杂度为O(n
)，其中n为元素数量，因此在数据量较大的情况下性能较差。

#### 自适应搜索

自适应搜索利用数据的特有属性（例如有序性）来优化搜索过程，从而更高效地定位目标元素。
- “二分查找”利用数据的有序性实现高效查找，仅适用于数组。
- “哈希查找”利用哈希表将搜索数据和目标数据建立为键值对映射，从而实现查询操作。
- “树查找”在特定的树结构（例如二叉搜索树）中，基于比较节点值来快速排出节点，从而定位目标元素。

此类算法的优点是效率高，时间复杂度可达到O(logn)甚至O(1)。

然而，使用这些算法需要对数据进行预处理。例如，二分查找需要预先对数组进行排序，哈希查找和树查找需要借助额外的数据结构，维护这些数据结构也需要额外的时间和空间开支。

#### 搜索方法选取

实际应用中，我们需要对数据体量、搜索性能要求、数据查询和更新频率等因素进行具体分析，从而选择合适的搜索方法。

**线性搜索**
- 通用性好，无需任何数据预处理操作。加入仅需查询一次数据，那么其他三种方法的预处理的时间比线性搜索的时间还要更长。
- 适用于体量较小的数据，此情况下的时间复杂度对效率影响较小。
- 适用于数据更新频率较高的场景，因为该方法不需要对数据进行任何额外维护。

**二分查找**
- 适用于大数据量的情况，效率表现稳定，最差时间复杂度为O(logn)。
- 数量高不能过大，因为存储数组需要连续的内存空间。
- 不适用于高频增删数据的场景，因为维护有序数组的开销较大。

**哈希查找**
- 适合对查询性能要求很高的场景，平均时间复杂度为O(1)。
- 不适合需要有序数据或范围查找的场景，因为哈希表无法维护数据的有序性。
- 对哈希函数和哈希冲突处理的依赖性较高，具有较大的性能劣化风险。
- 不适合数据量过大的情况，因为哈希表需要额外空间来最大程度的减少冲突，从而提供良好的查询性能。

**树查找**
- 适用于海量数据，因为树节点在内存中是分散存储的。
- 适合需要维护有序数据或范围查找的场景。
- 在持续增删节点的过程中，二叉搜索树可能产生倾斜，时间复杂度劣化至O(n)。
- 若使用AVL或红黑树，则各项操作可在O(logn)效率下稳定运行，但维护树平衡的操作会增加额外开销。

用哈希表替换线性查找是一种常用的优化运行时间的策略，可将时间复杂度从O(n)降至O(1)。

## 排序

### 排序算法

「排序算法」用于对一组数据按照特定顺序进行排列。

#### 评价维度

- **运行效率**：我们期望排序算法的时间复杂度尽量低，总体操作数量较少（即时间复杂度中的常数项较低）。对大数量情况，运行效率显得尤为重要。
- **就地性**：顾名思义，「原地排序」通过在原数组上直接操作实现排序，无需借助额外的辅助数组，从而节省内存。通常情况下，原地排序数据搬运操作较少，运行速度也更快。
- **稳定性**：「稳定排序」在完成排序后，相等元素在数组中的相对顺序不发生改变。

稳定排序是多级排序场景的必要条件。假设我们有一个存储学生信息的表格，第 1 列和第 2 列分别是姓名和年龄。在这种情况下，「非稳定排序」可能导致输入数据的有序性丧失。

```
# 输入数据是按照姓名排序好的
# (name, age)
  ('A', 19)
  ('B', 18)
  ('C', 21)
  ('D', 19)
  ('E', 23)

# 假设使用非稳定排序算法按年龄排序列表，
# 结果中 ('D', 19) 和 ('A', 19) 的相对位置改变，
# 输入数据按姓名排序的性质丢失
  ('B', 18)
  ('D', 19)
  ('A', 19)
  ('C', 21)
  ('E', 23)
```
- **自适应性**：「自适应排序」的时间复杂度受输入数据的影响，即最佳、最差、平均时间复杂度并不完全相等；

自适应性需要根据具体情况来评估。如果最差时间复杂度差于平均时间复杂度，说明排序算法在某些数据下性能可能劣化，因此被视为负面属性。而如果最佳时间复杂度优于平均时间复杂度，则被视为正面属性。

- **是否基于比较**：「基于比较的排序」依赖于比较运算符（>=<）来判断元素的相对顺序，从而排序整个数组，理论最优时间复杂度是O(nlogn)。而「非比较排序」不使用比较运算符，时间复杂度可达O(n)，但其通用型相对较差。

#### 理想排序算法

运行快、原地、稳定、正向自适应、通用性好。

### 选择排序
#### 概念
「选择排序 selection sort」的工作原理非常直接：开启一个循环，每轮从未排序区间选择最小的元素，将其放到已排序区间的末尾。

设数组的长度为n，选择排序的算法流程如图 11-2 所示。

1. 初始状态下，所有元素未排序，即未排序（索引）区间为\[0,n-1\]。
2. 选取区间\[0,n-1\]中的最小元素，将其与索引0处元素交换。完成后，数组前 1 个元素已排序。
3. 选取区间\[0,n-1\]中的最小元素，将其与索引1处元素交换。完成后，数组前 2 个元素已排序。
4. 以此类推。经过n-1轮选择与交换后，数组前n-1个元素已排序。
5. 仅剩的一个元素必定是最大元素，无须排序，因此数组排序完成。

#### 代码实现：
```
function selectionSort(nums) {
    let n = nums.length;
    // 外循环：未排序区间为 [i, n-1]
    for(let i = 0; i < n - 1; i++) {
        // 内循环：找到未排序区间内的最小元素
        let k = i;
        for(let j = i; j < n; j++) {
            if(nums[j] < nums[k]) {
                k = j;  // 记录最小元素的索引
            }
        }
        // 将该最小元素与未排序区间的首个元素交换
        let tmp = nums[i];
        nums[i] = nums[k];
        nums[k] = tmp;
    }
    return nums;
}
```

#### 特性
- 时间复杂度O(n^2)，非自适应排序；
- 空间复杂度O(1)，原地排序；
- 非稳定排序：元素 nums[i] 有可能被交换至与其相等的元素的右边，导致两者相对顺序发生改变。

### 冒泡排序
#### 概念
「冒泡排序 bubble sort」通过连续地比较与交换相邻元素实现排序。这个过程就像气泡从底部升到顶部一样，因此得名冒泡排序。

设数组的长度为n，冒泡排序的步骤如下。

1. 首先，对n个元素执行“冒泡”，将数组的最大元素交换至正确位置，
2. 接下来，对剩余n-1个元素执行“冒泡”，将第二大元素交换至正确位置。
3. 以此类推，经过n-1轮“冒泡”后，前n-1大的元素都被交换至正确位置。
4. 仅剩的一个元素必定是最小元素，无须排序，因此数组排序完成。

#### 代码实现
```
/** 冒泡排序 */
function bubbleSort(nums) {
    let n = nums.length;
    // 外循环：未排序区间为 [0, i]
    for(let i = n - 1; i > 0; i--) {
        // 内循环：将未排序区间 [0, i] 中的最大元素交换至该区间的最右端
        for(let j = 0; j < i; j++) {
            if(nums[j] > nums[j+1]) {
                // 交换 nums[j] 与 nums[j + 1]
                let tmp = nums[j];
                nums[j] = nums[j+1];
                nums[j+1] = tmp;
            }
        }
    }
    return nums;
}
```

#### 效率优化

如果某轮“冒泡”中没有执行任何交换操作，说明数组已完成排序，可直接返回结果。因此，可以增加一个标志位flag来检测这种情况，一旦出现就立即返回。

经过优化，冒泡排序的最差和平均时间复杂度仍为O(n^2)；但当输入数组完全有序时，可达到最佳时间复杂度O(n)。

```
/** 冒泡排序（标志优化） */
function bubbleSortWithFlag(nums) {
    let n = nums.length;
    // 外循环：未排序区间为 [0, i]
    for(let i = n - 1; i > 0; i--) {
        let flag = false; // 初始化标志位
        // 内循环：将未排序区间 [0, i] 中的最大元素交换至该区间的最右端
        for(let j = 0; j < i; j++) {
            if(nums[j] > nums[j+1]) {
                // 交换 nums[j] 与 nums[j + 1]
                let tmp = nums[j];
                nums[j] = nums[j+1];
                nums[j+1] = tmp;
                flag = true; // 记录交换元素
            }
        }
        if(!flag) break; // 此轮冒泡未交换任何元素，直接跳出
    }
    return nums;
}
```

#### 特性
- 时间复杂度O(n^2)，自适应排序：引入flag优化后，最佳时间复杂度可达到O(n)；
- 空间复杂度O(1)，原地排序；
- 稳定排序：在“冒泡”中遇到相等元素不交换；

### 插入排序
#### 概念
「插入排序 insertion sort」是一种简单的排序算法，它的工作原理与手动整理一副牌的过程非常相似。

插入排序的整体流程如下：

1. 初始状态下，数组的第 1 个元素已完成排序。
2. 选取数组的第 2 个元素作为 base ，将其插入到正确位置后，数组的前 2 个元素已排序。
3. 选取第 3 个元素作为 base ，将其插入到正确位置后，数组的前 3 个元素已排序。
4. 以此类推，在最后一轮中，选取最后一个元素作为 base ，将其插入到正确位置后，所有元素均已排序。

#### 代码实现
```
/** 插入排序 */
function insertionSort(nums) {
    let n = nums.length;
    // 外循环：已排序元素数量为 1, 2, ..., n
    for(let i = 0; i < n; i++) {
        let base = nums[i];
        // 用二分法将base插入到有序数组里
        let j = i - 1;
        // 内循环：将base插入到已排序不分的正确位置
        while(j > 0 && nums[j] > base) {
            nums[j+1] = nums[j]; // 将数组整体后移，给base挪位置
            j--;
        }
        nums[j+1] = base; // 将base赋值到正确位置
    }
    return nums;
}
```

#### 特性
- 时间复杂度O(n^2)，自适应排序： 当遇到有序数据时，插入操作会提前终止，插入排序达到最佳时间复杂度O(n)；
- 空间复杂度O(1)，原地排序；
- 稳定排序：在插入操作过程中，会将元素插入到相等元素的右侧，不会改变它们的顺序。

#### 插入排序优势

插入排序的时间复杂度（O(n^2)）比快速排序更高（O(nlogn)），但在小数据量的情况下，插入排序通常更快。

这个结论与线性查找与二分查找的使用情况的结论类似。快速排序属于基于分治的排序算法，往往包含更多的单元计算操作，数据量较小时，n^2与nlogn的数值比较接近，时间复杂度不占主导作用，每轮中的单元操作的数量起到决定性因素。

许多编程语言的内置排序函数都采用插入排序，大致思路为：对于长数组，采用基于分治的排序算法，例如快速排序；对于短数组，直接使用插入排序。

插入排序的使用频率更高，主要有以下原因：
- 冒泡排序基于元素交换实现，需要借助临时变量，共涉及3个单元操作；插入排序基于元素赋值实现，仅需1个单元操作，冒泡排序的计算开销通常比插入排序更高。
- 如果给定一部分有序的数据，插入排序比选择排序的效率更高。
- 选择排序不稳定，无法用于多级排序。

### 快速排序
#### 概念

「快速排序 quick sort」是一种基于分治策略的排序算法，运行高效，应用广泛。

快速排序的核心操作是“哨兵划分”，其目标是：选择数组中的某个元素作为“基准数”，将所有小于基准数的元素移到其左侧，而大于基准数的元素移到其右侧。具体来说，哨兵划分的流程如下：

1. 选取数组最左端元素作为基准数，初始化两个指针 i 和 j 分别指向数组的两端。
2. 设置一个循环，在每轮中使用 i（j）分别寻找第一个比基准数大（小）的元素，然后交换这两个元素。
3. 循环执行步骤 2. ，直到 i 和 j 相遇时停止，最后将基准数交换至两个子数组的分界线;

哨兵划分的实质是将一个较长数组的排序问题简化为两个较短数组的排序问题。

#### 算法流程
快速排序的整体流程如下：

1. 首先，对原数组执行一次“哨兵划分”，得到未排序的左子数组和右子数组。
2. 然后，对左子数组和右子数组分别递归执行“哨兵划分”。
3. 持续递归，直至子数组长度为 1 时终止，从而完成整个数组的排序。

#### 代码实现
```

```
#### 特性
- **时间复杂度O(nlogn)、自适应排序**：在平均情况下，哨兵划分的递归层数为logn，每层中的总循环数为n，总体使用时间O(nlogn)。在最差情况下，每轮哨兵划分操作都将长度为n的数组划分为长度为0和 n-1的两个子数组，此时递归层数达到n层，每层中的循环数为n，总体使用 
O(n^2)时间。
- **空间复杂度O(n)、原地排序**：在输入数组完全倒序的情况下，达到最差递归深度n，使用O(n)栈帧空间。排序操作是在原数组上进行的，未借助额外数组。
- **非稳定排序**：在哨兵划分的最后一步，基准数可能会被交换至相等元素的右侧。

#### 快排块在哪里？

快速排序的平均时间复杂度与“归并排序”和“堆排序”相同，但通常快速排序的效率更高，主要原因如下：
- **出现最差情况的概率很低**：虽然快速排序的最差时间复杂度为O(n^2)，没有归并排序稳定，但在绝大多数情况下，快速排序能在O(nlogn)的时间复杂度下运行。
- **缓存使用效率高**：在执行哨兵划分操作时，系统可将整个子数组加载到缓存，因此访问元素的效率较高。而像“堆排序”这类算法需要跳跃式访问元素，从而缺乏这一特性。
- **复杂度的常数系数低**：与“归并排序”和“堆排序”相比，快速排序的比较、赋值
交换等操作的总数量最少。

#### 基准数优化
快速排序在传入倒序数组时，分治策略失效，会退化为“冒泡排序”。

为了避免这种情况发生，可以优化哨兵划分中的基准数的选取策略。可以在数组中选取三个候选元素（通常为数组的首、尾、中点元素），并将这三个候选元素的中位数作为基准数。这样一来，基准数“既不太大也不太小”的概率将大幅提升，采用这种方式后，时间复杂度劣化至O(n^2)的概率大大降低。
#### 尾递归优化（没看懂，后面再来看）
在某些输入下，快速排序kennel占用空间较多。以完全倒序的输入为例，由于每轮哨兵划分后右子数组长度为0，递归树的高度达到n-1，需要占用O(n)大小的堆栈空间。

为了防止堆栈空间的累积，我们可以在每轮哨兵排序完成后，比较两个子数组的长度，仅对较短的子数组进行递归。由于较短子数组的长度不会超过 n/2，因此这种方法能确保递归深度不超过logn，从而将最差空间复杂度优化至O(logn)。

```
/* 快速排序（尾递归优化） */
quickSort(nums, left, right) {
    // 子数组长度为 1 时终止
    while (left < right) {
        // 哨兵划分操作
        let pivot = this.partition(nums, left, right);
        // 对两个子数组中较短的那个执行快排
        if (pivot - left < right - pivot) {
            this.quickSort(nums, left, pivot - 1); // 递归排序左子数组
            left = pivot + 1; // 剩余未排序区间为 [pivot + 1, right]
        } else {
            this.quickSort(nums, pivot + 1, right); // 递归排序右子数组
            right = pivot - 1; // 剩余未排序区间为 [left, pivot - 1]
        }
    }
}
```

### 归并排序
#### 概念
「归并排序」是一种基于分治策略的排序算法，包含“划分”和“合并”阶段。

1. **划分阶段**：通过递归不断将数组从中点处分开，将长数组的排序问题转换为短数组的排序问题。
2. **合并阶段**：将子数组长度为1时终止划分，开始合并，持续的将左右两个较短的有序数组合并为一个较长的有序数组，直至结束。

#### 算法流程
“划分阶段”从顶至底递归地将数组从中点切分为两个子数组。

1. 计算数组中点 mid ，递归划分左子数组（区间 [left, mid] ）和右子数组（区间 [mid + 1, right] ）。
2. 递归执行步骤 1. ，直至子数组区间长度为 1 时，终止递归划分。

“合并阶段”从底至顶地将左子数组和右子数组合并为一个有序数组。需要注意的是，从长度为 1 的子数组开始合并，合并阶段中的每个子数组都是有序的。

归并排序和二叉树后续遍历的递归顺序是一致的：
- **后序遍历**：先递归左子树，后递归右子树，最后处理根节点。
- **归并排序**：先递归左子数组，后递归右子数组，最后处理合并。

#### 代码实现
```

```
#### 特性
- **时间复杂度O(nlogn)、非自适应排序**：划分产生高度为logn的递归树，每层合并的总操作数量为n，因此总体时间复杂度为O(nlogn)。
- **空间复杂度O(n)、非原地排序**：递归深度为logn，使用logn大小的栈帧空间。合并操作需要借助辅助数组实现，使用O(n)大小的额外空间。
- **稳定排序**：在合并过程中，相等元素的次序保持不变。

### 堆排序
#### 概念
「堆排序 heap sort」是一种基于堆数据结构实现的高效排序算法。我们可以利用已经学过的“建堆操作”和“元素出堆操作”实现堆排序。

1. 输入数组并建立小顶堆，此时最小元素位于堆顶。
2. 不断执行出堆操作，依次记录出堆元素，即可得到从小到大排序的序列。

#### 算法流程
设数组的长度为n，堆排序的流程如下：

1. 输入数组并建立大顶堆。完成后，最大元素位于堆顶。
2. 将堆顶元素（第一个元素）与堆底元素（最后一个元素）交换。完成交换后，堆的长度减1，已排序元素数量加 
1。
3. 从堆顶元素开始，从顶到底执行堆化操作（Sift Down）。完成堆化后，堆的性质得到修复。
4. 循环执行第 2. 和 3. 步。循环n-1轮后，即可完成数组排序。

#### 代码实现
```
/** 堆的长度为n，从节点i开始，从顶至底堆化 */
function shiftDown(nums, n, i) {
    while(true) {
        // 判断i，l，r中的最大值，记为ma
        let l = 2 * i + 1;
        let r = 2 * i + 2;
        let ma = i;
        if(l < n && nums[l] > nums[ma]) {
            ma = l;
        }
        if(r < n && nums[r] > nums[ma]) {
            ma = r;
        }
        // 若节点i最大，或索引l、r越界，则无需继续堆化，跳出
        if(ma === i)  break;
        // 交换两节点
        [nums[i], nums[ma]] = [nums[ma], nums[i]];
        // 循环向下堆化
        i = ma;
    }
}
/** 堆排序 */
function heapSort() {
    // 建堆操作：堆化除叶节点以外的其他所有节点
    for(let i = Math.floor(nums.length / 2) - 1; i >= 0; i--) {
        shiftDown(nums, num.length, i);
    }
    for(let i = nums.length - 1; i >= 0; i--) {
        // 交换根节点与最右叶节点
        [nums[0], nums[i]] = [nums[i], nums[0]];
        // 以根节点为起点，从顶至底进行堆化
        shiftDown(nums, i, 0);
    }
}
```
#### 特性
- **时间复杂度O(logn)、非自适应排序**：建堆操作使用O(n)时间。从堆中提取最大元素的时间复杂度为 O(logn)，共循环n-1轮。
- **空间复杂度O(1)、原地排序**：几个指针变量使用O(1)空间。元素交换和堆化操作都是在原数组上进行的。
- **非稳定排序**：在交换堆顶元素和堆底元素时，相等元素的相对位置可能发生变化。

### 桶排序
#### 概念
「桶排序 bucket sort」是分治策略的一个典型应用。它通过设置一些具有大小顺序的桶，每个桶对应一个数据范围，将数据平均分配到各个桶中；然后，在每个桶内部分别执行排序；最终按照桶的顺序将所有数据合并。

#### 算法流程
考虑一个长度为n的数组，元素是范围\[1, 0\)的浮点数。桶排序的流程如下：

1. 初始化k个桶，将n个元素分配到k个桶中。
2. 对每个桶分别执行排序（本文采用编程语言的内置排序函数）。
3. 按照桶的从小到大的顺序，合并结果。
#### 代码实现
```
/** 桶排序 */
function bucketSort(nums) {
    // 初始化k = n/2 个桶，预期向每个桶分配2个元素
    const k = nums.length / 2;
    const buckets = [];
    for(let i = 0 ; i < k; i++) {
        buckets.push([]);
    }  
    // 1. 将数组元素分配到各个桶中
    for(const num of nums) {
        // 输入数据范围[0, 1)，使用num * k映射到索引范围[0, k-1]
        const i = Math.floor(num * k);
        // 将num添加进桶i
        buckets[i].push(num);
    }  
    // 2. 对各个桶执行排序
    for(const bucket of buckets) {
        // 使用内置排序函数，也可以替换成其他排序算法
        bucket.sort((a,b) => a - b);
    } 
    // 3. 遍历桶合并结果
    let i = 0;
    for(const bucket of buckets) {
        for(const num of bucket) {
            nums[i++] = num;
        }
    }
}
```
#### 特性

桶排序适用于处理体量很大的数据。例如，输入数据包含 100 万个元素，由于空间限制，系统内存无法一次性加载所有数据。此时，可以将数据分成 1000 个桶，然后分别对每个桶进行排序，最后将结果合并。

- **时间复杂度O(n + k)** ：假设元素在各个桶内平均分布，那么每个桶内的元素数量为n/k。假设排序单个桶使用O(n/k * log n/k)时间，则排序所有桶使用时间O(nlog n/k)。当桶数量k比较大时，时间复杂度则趋向于O(n)。合并结果时需要遍历所有桶和元素，花费O(n + k)时间(**这里没明白，需要再看看**)。
- **自适应排序**：在最坏情况下，所有数据被分配到一个桶中，且排序该桶使用O(n^2)时间。
- **空间复杂度O(n + k)、非原地排序**：需要借助k个桶和总共n个元素的额外空间。
- 桶排序是否稳定取决于排序桶内元素的算法是否稳定。

#### 如何实现平均分配

桶排序的时间复杂度理论上可以到O(n)，关键在于元素均匀分配到各个桶中，因为实际数据往往不是均匀分布的。例如淘宝上所有商品的价格范围，若将价格区间平均划分成10份，各个桶中的商品数据差距会非常大。

为了实现平均分配，我们可以设置一个大致的分界点，先将数据粗略划分到3个桶中，再将商品较多的桶继续划分。如果知道商品价格的概率分布，就可以根据数据概率分布设置每个桶的价格分界线，从而将商品平均分配到每个桶中。

### 计数排序
#### 概念
「计数排序 counting sort」通过统计元素数量来实现排序，通常应用于整数数组。

#### 算法流程
给定一个长度为n的数组 nums ，其中的元素都是“非负整数”，计数排序的整体流程如下：

1. 遍历数组，找出数组中的最大数字，记为m，然后创建一个长度为m+1的辅助数组 counter 。
2. 借助 counter 统计 nums 中各数字的出现次数，其中 counter[num] 对应数字 num 的出现次数。统计方法很简单，只需遍历 nums（设当前数字为 num），每轮将 counter[num] 增加1即可。
3. 由于 counter 的各个索引天然有序，因此相当于所有数字已经被排序好了。接下来，我们遍历 counter ，根据各数字的出现次数，将它们按从小到大的顺序填入 nums 即可。

#### 代码实现
```
/** 计数排序 */
function countingSortNaive(nums) {
    // 1. 统计数组中最大元素m
    let max = nums[0];
    for(const num of nums) {
        if(nums[i] > max) {
            max = nums[i]
        }
    }
    // 2. 统计各数字的出现次数
    // counter[num]代表num的出现次数
    const counter = new Array(m+1).fill(0);
    for(let i = 0; i < nums.length; i++) {
        counter[nums[i]]++;
    }
    // 3. 遍历counter，将个元素填入原数组nums
    let i= 0;
    for(let num = 0; num < m + 1; num++) {
        for(let j = 0; j < counter[num]; j++) {
            nums[i] = num;
            i++;
        }
    }
}
```

> 从桶排序的角度看，我们可以将计数排序中的计数数组 counter 的每个索引视为一个桶，将统计数量的过程看作是将各个元素分配到对应的桶中。本质上，计数排序是桶排序在整型数据下的一个特例。

#### 特性
- **时间复杂度O(m+n)**：涉及遍历 nums 和遍历 counter ，都使用线性时间。一般情况下n远大于m，时间复杂度趋于O(n)。
- **空间复杂度O(m+n)、非原地排序**：借助了长度分别为n和m的数组 res 和 counter 。
- **稳定排序**：由于向 res 中填充元素的顺序是“从右向左”的，因此倒序遍历 nums 可以避免改变相等元素之间的相对位置，从而实现稳定排序。实际上，正序遍历 nums 也可以得到正确的排序结果，但结果是非稳定的。

#### 局限性
看到这里，你也许会觉得计数排序非常巧妙，仅通过统计数量就可以实现高效的排序工作。然而，使用计数排序的前置条件相对较为严格。

- **计数排序只适用于非负整数。**若想要将其用于其他类型的数据，需要确保这些数据可以被转换为非负整数，并且在转换过程中不能改变各个元素之间的相对大小关系。例如，对于包含负数的整数数组，可以先给所有数字加上一个常数，将全部数字转化为正数，排序完成后再转换回去即可。

- **计数排序适用于数据量大但数据范围较小的情况。**比如，在上述示例中m不能太大，否则会占用过多空间。而当n远小于m时，计数排序使用O(m)时间，可能比O(nlogn)的排序算法还要慢。

### 基数排序
#### 概念
「基数排序 radix sort」的核心思想与计数排序一致，也通过统计个数来实现排序。在此基础上，基数排序利用数字各位之间的递进关系，依次对每一位进行排序，从而得到最终的排序结果。

#### 算法流程
以学号数据为例，假设数字的最低位是第1位，最高位是第 
8位，基数排序的流程如下：

1. 初始化位数k=1。
2. 对学号的第k位执行“计数排序”。完成后，数据会根据第k位从小到大排序。
3. 将k增加1，然后返回步骤 2. 继续迭代，直到所有位都排序完成后结束。
#### 代码实现
```

```
#### 特性

相较于计数排序，基数排序适用于数值范围较大的情况，但前提是数据必须可以表示为固定位数的格式，且位数不能过大。例如，浮点数不适合使用基数排序，因为其位数 
 过大，可能导致时间复杂度 
 。

- **时间复杂度O(nk)**：设数据量为n、数据为d进制、最大位数为k，则对某一位执行计数排序使用O(n+d)时间，排序所有k位使用O((n+d)k)时间。通常情况下，d和k都相对较小，时间复杂度趋向O(n)。
- **空间复杂度O(n+d)、非原地排序**：与计数排序相同，基数排序需要借助长度为n和d的数组 res 和 counter 。
- **稳定排序**：与计数排序相同。